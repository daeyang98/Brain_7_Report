{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA3888/PHYS3888 Interdisciplinary Report: Space Run\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Executive Summary\n",
    "\n",
    "In the interdisciplinary unit of DATA3888/PHYS3888 (Semester 1, 2021), our team aimed to build a brain machine interface that uses machine-learning techniques to gain meaningful insights from real-time physiological measurements. In an increasingly data-driven world, there is a demand for data scientists to effectively combine, analyse and derive observations from large datasets (University of Sydney, 2021). Physicists are also required to adapt from their traditional qualitative-grounded science and apply their expertise more broadly in this data-centric era in order to contribute to modern day problems (University of Sydney, 2021). Through this interdisciplinary project, Brain_7 has introduced an innovative solution that  illustrates the concepts and skills we have learnt from our respective disciplines. In addition, the dual disciplinary team helmed by Data Science and Physics majors, worked together to utilise our strengths to a collaborative advantage. \n",
    "\n",
    "Our focus for Brain_7 was to design a solution that solved a modern-day social problem. Over 4.3 million Australians are estimated to have a form of disability, and over three-quarters (76.8%) of those living with a disability reported a physical disorder as their main condition (Australian Network of Disability, 2019). With these factors taken into consideration, our aim for this project was not only to demonstrate our interdisciplinary effectiveness but also, to conceptualise a solution that would actively engage the population living with mobility issues. \n",
    "\n",
    "The solution that we have created is Space Run, a game inspired by Space Invaders and Temple Run. The Physics students used the Brain Spikerbox and the EEG headband to detect electrical activity caused by eye movement changes. These measurements were then extracted by the Data students using a simple classifier which assessed and then categorised the eye movements into left and right. The decisions made by the simple classifier were fed into the application, allowing a user to play our creative yet simple game, Space Run. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Aim & Methods \n",
    "\n",
    "### 2.1 Data Collection\n",
    "\n",
    "**Aim**: *To obtain data for accuracy tests.*\n",
    "\n",
    "**Applications/Equipment**:\n",
    "* EEG Headband x1\n",
    "* SpikerBox x1\n",
    "    * USB cable x1\n",
    "    * Red alligator clips x2\n",
    "    * Black alligator clip x1\n",
    "    * 9V battery x1\n",
    "* Electrode Pad x1\n",
    "* SpikeRecorder App \n",
    "* Python \n",
    "\n",
    "**Method**: *Obtaining data using SpikeRecorder/Python.*\n",
    "\n",
    "1. The EEG headband was worn around the head. The two electrodes on the band were placed directly above the eyebrow and parallel to the eye.\n",
    "2. An electrode pad was placed behind the ear on the bone.\n",
    "3. A 9V battery was placed in the SpikerBox. One end of the three alligator clips were also connected to the SpikerBox. The SpikerBox was then connected to a computer with the USB cable. \n",
    "4. The two red alligator clips were then connected to each electrode on the headband and the black alligator clip connected to the electrode on the neck.\n",
    "5. The SpikeRecorder app was opened and the SpikerBox turned on. Or otherwise Python was opened and the script ‘saver.py’ ran.\n",
    "6. In SpikeRecorder settings the band pass filter was set to 8-15Hz and the notch filter set to 50Hz.\n",
    "7. We recorded 3 consecutive left eye-movements. The recording and the annotation file was saved.\n",
    "8. Step 7 was repeated 10 times. Each repetition was labeled numerically ie. The first recording and annotation were labeled 3L_1.\n",
    "9. We then recorded 3 consecutive right eye-movements. The recording and the annotation file was saved.\n",
    "10. Step 9 was repeated 10 times. Each repetition was labeled numerically ie. The first recording and annotation was labeled 3R_1.\n",
    "\n",
    "**Method**: *Performing Left and Right Eye-Movements.*\n",
    "1. The wearer of the EEG headband remained seated, sitting still and in an upright position. They were told to refrain from speaking or moving any part of their body other than their eyes.\n",
    "2. Eye’s began by looking straight ahead.\n",
    "3. For a left eye-movement they were told “left”. The operator of the SpikeRecorder app then pressed the spacebar simultaneously so an annotation would be recorded of when the movement occurred.\n",
    "4. To move left they moved their eyes quickly from the central resting point to the furthest left point and then quickly back to central position in horizontal motion.\n",
    "5. Three consecutive left eye movements were recorded in a single file with each movement occurring 3 seconds after the previous.\n",
    "6. Steps 1 to 5 were repeated when recording right eye-movements. However they were cued to move with a verbal “right” and looked to the farthest right point on the horizontal.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Aim**: *To filter and standardise the data on Python.*\n",
    "\n",
    "\n",
    "**Method**: The raw data read from the SpikerBox was kept in a moving buffer. This buffer gets standardised, notch filtered at 50 Hz (electronic noise from mains power supply) and then downsampled to ~100 samples before being passed into a classifier model to be classified. The downsampling is done to ensure the classification is consistent across varying buffer sizes and also to increase performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Developed Model \n",
    "#### 2.2.1 Simple Classifier \n",
    "\n",
    "**Aim**: *To detect eye movements.*\n",
    "\n",
    "**Method**: The zero-crossing heuristic is used to detect eye movements. Based on the data collected, it can be seen that when there is an eye movement, values tend to be either consecutively positive or consecutively negative, indicating that adjacent values do not cross zero, which means their product is greater than zero. In contrast, when there is no eye movement, values tend to hover around zero so adjacent values are likely to be of different signs, and thus their product is less than zero. Therefore, within a certain time interval, if the number of zero-crossing events exceeds a certain threshold, there is no eye movement, otherwise there is an eye movement. To determine the size of the time interval and the threshold for the number of zero-crossing, an optimization algorithm was used which was an aim that will be discussed later. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aim**: *To identify left and right eye movements.*\n",
    "\n",
    "**Method**: Once an eye movement is detected, whether it is left or right needs to be determined. It is obvious that the order of peak and trough was opposite for left and right eye movements. For a left eye movement, it is firstly a trough and then a peak while for a right eye movement, it is a peak followed by a trough. This allows left and right to be decided based on determining which comes first, the maximum or minimum value. If the minimum is before the maximum, it is a left eye movement, otherwise it is a right eye movement. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aim**: *To prevent misidentification of events.*\n",
    "\n",
    "**Method**:To prevent misidentifying noise as events, there were restrictions for minimum amplitude. Noise could also have some values that are away from zero but usually these values are not as large as the maximum value of an eye movement, nor as small as the minimum value of an eye movement, in other words, the amplitude of noise is not large enough. Thus restricting a minimum amplitude helps filter noise. \n",
    "\n",
    "To prevent the gap between two consecutive events from being mistaken for an event, there were restrictions for maximum spacing between peak and trough. If a time interval includes the second half of a previous event and the first half of a subsequent event, this interval may be misidentified as having an eye movement. For example, two left movements were made and there is a time interval between them including the second half of the first left movement, a peak, and the first half of the second left movement, a trough and then this time interval could be mistaken for an eye movement. According to the fact that in a real eye movement the spacing is small because there is no stopping while moving eyes, the maximum spacing between peak and trough is restricted to avoid the misidentification. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aim**: *To optimize the classifier.*\n",
    "\n",
    "**Method**: for simple classifier: In order to optimise the classifier, the most appropriate values need to be found for the parameters. Firstly, an accuracy measurement function is made to return 1 - accuracy. Then an optimisation algorithm scipy.optimize.minimize() which is based on the nelder-mead algorithm was used to minimise the accuracy measurement function. Basically it computes the accuracy measurement function given an initial set of parameters and uses some algorithms to determine the next set of values to test and so on. Then it chooses the set of parameters that minimises the function. As a result,  parameters that maximized the accuracy of the classifier were returned. \n",
    "The parameters that were considered to be optimized and their values were as follows: \n",
    "* Buffer time (amount of time represented in the moving buffer): 1.462\n",
    "* Update factor (the fraction of the moving buffer read in through the stream): 0.1\n",
    "* Wait time (wait time after classification): 0\n",
    "* Number of samples (to downsample before classification): 106\n",
    "* Quality factor (parameter used in notch filter): 1\n",
    "* Event threshold (the threshold for the number of zero-crossing): 101\n",
    "* Positive amplitude threshold: 15\n",
    "* Negative amplitude threshold: 15\n",
    "* Spacing (maximal spacing between peak and trough as a fraction of number of samples): 0.206\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Complex Classifier\n",
    "**Aim**: *To determine whether a complex classifier is more suitable than the simple classifiers.*\n",
    "\n",
    "**Method**: The complex classifier uses features computed by the catch22 library to train on samples. These samples were created from SpikerBox recordings using an annotations file that contains timestamps to when the eye movements occurred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluation Strategies \n",
    "**Aim**: *To evaluate the accuracies of classifiers.*\n",
    "\n",
    "**Method**: A total of 23 test files were recorded using `saver.py`. 22 of which consisted of around 3 left or 3 right eye movements with the last file being a 2 minute long recording of around 33 events. Using 10-fold cross validation the accuracies per fold to produce a boxplot for each classifier. The accuracy used was the prediction accuracy of the classifier which is the percentage of predictions made by the classifier that were correct i.e. corresponded to an actual event as described by the annotation files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aim**: *To evaluate the computational time (lagging) of classifiers.*\n",
    "\n",
    "**Method**: The time taken for a classifier to return a result given processed data was recorded for all 5 models on one single test file for 1000 trials. This data produced a box plot as shown below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Justification \n",
    "We have experimented with different classifiers such as the Modified Simple Classifier (MSC) Unmodified Simple Classifier (USC), K-Nearest Neighbours (KNN), Random Forest Classifier (RFC) and C Support Vector Classification (SVC). Our two main criteria for evaluating each classifier was the accuracy rate and the time it took to classify the eye movements from left and right. We prioritised accuracy and hence evaluated each model in terms of the accuracy rate they produced. The boxplot of accuracies produced above was done through a 10-fold cross validation to train and test each respective model. Each model was then evaluated against a test dataset to produce the box plots of weighted average accuracy. In addition, as shown in Figure 1, \n",
    "down-sampling increases performance across all models of classification. However, for the simple classifiers it was difficult to observe a noticeable difference by down-sampling as the time it took to classify was already extremely short (<1ms). \n",
    "\n",
    "The USC gave an approximate median accuracy of 30% which was similarly seen in KNN. Both models had a slight skew towards the right and had a moderate dispersion. These were the two lowest accurate models and were hence not considered further for the deployment of our project. SVC gave a slightly higher median accuracy of approximately 50% but as shown in the Figure 2 but had a much greater spread in the accuracies recorded which caused it to be unreliable and was not considered further in terms of accuracy. RFC gave a higher median accuracy of approximately 70% and had a slight left skew whilst MSC gave a 100% accuracy rate across for most snippets of data. However, contrary to expectations, the outlier noted was due to the extended recording of eye movements.\n",
    "\n",
    "We took into consideration the two models RFC and MSC which had the highest median accuracies initially. After further discussion, we have chosen to go with MSC as the classification time for RFC was undoubtedly higher with a median of 8ms compared to MSC which classified in under 1ms as shown in the Figure 3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[insert 2 boxplot here] figure 2 is accuracy figure 3 is classifcation time, figure 1 is down sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Deployment Process \n",
    "In Figure 4 shown below, depicts a step-by-step process of how Space Run works. Through the lens of a Physics disciplinary perspective, the changes in electrical impulse when there are spontaneous changes in eye movement are detected by electrodes in the EEG headband which are connected to the SpikerBox. These readings are then fed into the computer. In line with data-science methodology, the computer then sends the raw data to a moving buffer, a window size of approximately 1.5 seconds.  The readings are then standardized by applying a 50 Hz notch filter and the classifier then determines whether the eye movements were left or right. This process is iterated continuously and hence allows the user to play the game by controlling the spaceship left and right. Whilst this process is ongoing the same computer displays and handles the game logic simultaneously.\n",
    "\n",
    "Space Run is a simple game where the user controls the spaceship left and right, collecting the blue pigments of energy and avoiding the red obstacles. There are three possible lanes and if a player is in the left most lane and signals left with their eye movements. The Spaceship loops around, like a torus, moving in the right most position. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[insert schematic diagram here + link to video of elsa?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion and Conclusion \n",
    "\n",
    "\n",
    "While our model was mostly accurate following our testing, there were some shortcomings that we identified that remain as areas that can be further improved upon. In terms of raw input, we collected a sample size of recordings with most of the group members in our team and noticed slight differences in each of them when observing the recorded files. This only goes to show the nuance and adjustments that our model would need to adapt to for there to be any consideration of commercial deployment. Since our final product does not have a calibration phase before playing the game, it is only a handful of the team members’ data that is being used on what could be a novel user. Adding a calibration phase would be ideal in bypassing the small sample size by using the phase as an opportunity to collect new training data for our model whilst also adjusting the model to the specific user. \n",
    "\n",
    "In terms of the classification that our model provides, an ideal feature to add is the ability to recognise and classify blinks. The game we developed utilises left and right eye-movement, with the objective being to collect and dodge colour-coded obstacles. Whilst entertaining upon inception, it could potentially get boring with time due to the complexity of the game being constant. Adding the ability to blink as a control for shooting oncoming obstacles could be a viable option that brings more depth to the game and adds to its value as a product for entertainment. However, this would not linearly get complex as the game progresses, hence, it would be worth considering finding ways in which the game’s difficulty can be scaled. There is a universal timekeeping variable in the game that controls the speed of oncoming obstacles, which could be modified to scale with time as the user progresses, thus creating a challenging environment where the user is entertained and focused. But, scaling the speed of the game has major potential for breaking our game as the model currently being used utilises a 1.5-second buffer in between the classification of events, and there would eventually be a point where the user needs to give input with an interval lesser than the buffer period. This would then require further readjustment of the model to create a shorter window.\n",
    "\n",
    "In conclusion, our product utilises a model that is robust in its accuracy as well as its efficiency when it comes to classifying eye movements. Our team was able to successfully implement a pipeline from the SpikerBox’s electrodes, with our physics team members finding effective ways of preprocessing the data with appropriate standardising and notch filtering, and in conjunction with the data science members developing a classification model trained on our team’s data and able to send input to the game’s logic with minimal latency, allowing the user to experience a game with accurate movements. For further future development, it would be ideal in finding more ways to make the game more complex and entertaining, whilst developing a method allowing for a shorter buffer period for our classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Contribution\n",
    "480452061: Generating data, writing the code, tuning the machine, writing/editing the report\n",
    "\n",
    "490423002:  Generating the data, tuning the machine, performing the processing, writing the code, making observations, deploying the final product and writing/editing the report\n",
    "\n",
    "480465692:  Generating the data, writing/editing the report\n",
    "\n",
    "490182143: Generating data, writing/editing the report\n",
    "\n",
    "490433791: Generating data, making observations, writing/editing report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References \n",
    "\n",
    "\n",
    "1. Australian Network On Disability (2019) Disability Statistics \n",
    "\n",
    "        https://www.and.org.au/pages/disability-statistics.html \n",
    "\n",
    "2. University of Sydney (2021) DATA3888: Data Science Capstone \n",
    "\n",
    "        https://www.sydney.edu.au/units/DATA3888 \n",
    "\n",
    "3. University of Sydney (2021) PHYS388: Physics Interdisciplinary Project \n",
    "\n",
    "        https://www.sydney.edu.au/units/PHYS3888 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
